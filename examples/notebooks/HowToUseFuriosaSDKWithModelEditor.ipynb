{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b067241e",
   "metadata": {},
   "source": [
    "# How to Use Furiosa SDK from Start to Finish with Processor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a5ecbcb",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use Furiosa SDK with `Processor`\n",
    "\n",
    "it is based on this [notebook](https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/HowToUseFuriosaSDKFromStartToFinish.ipynb) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5d1f6e7",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87b143f2",
   "metadata": {},
   "source": [
    "The Furiosa SDK needs to have been installed. If not, it can be installed following instructions on https://furiosa-ai.github.io/docs/latest/ko/ (Korean) or https://furiosa-ai.github.io/docs/latest/en/ (English). The `torchvision` and `scipy` packages also need to be installed for this demonstration.\n",
    "\n",
    "```console\n",
    "$ pip install 'furiosa-sdk[quantizer]' torchvision scipy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a65b9439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libfuriosa_hal.so --- v0.11.0, built @ 43c901f\n",
      "libfuriosa_hal.so --- v0.11.0, built @ 43c901f\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import onnx\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import tqdm\n",
    "\n",
    "from furiosa.optimizer import optimize_model\n",
    "from furiosa.quantizer import quantize, Calibrator, CalibrationMethod, ModelEditor, TensorType\n",
    "import furiosa.runtime.sync"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "318cb543",
   "metadata": {},
   "source": [
    "## Load PyTorch Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26114a1c",
   "metadata": {},
   "source": [
    "As a running example, we employ the pre-trained ResNet-50 model from Torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c157524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = torchvision.models.resnet50(weights=\"DEFAULT\")\n",
    "torch_model = torch_model.eval()  # Set the model to inference mode."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b228423",
   "metadata": {},
   "source": [
    "The ResNet50 model has been trained with the following preprocessing applied: https://pytorch.org/vision/stable/models.html We will use the same preprocessing for calibration and inference.\n",
    "\n",
    "Additionally, We will configure the process here to use image data as u8 type instead of converting it to f32 type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23cbe19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset preprocess for inference\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        # this part is different\n",
    "        transforms.PILToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# dataset preprocess for calibration\n",
    "calbirate_preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc90e0e9",
   "metadata": {},
   "source": [
    "## Export PyTorch Model to ONNX Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "995a571b",
   "metadata": {},
   "source": [
    "We call the `torch.onnx.export` function to export the PyTorch ResNet-50 model to an ONNX model. The function executes a PyTorch model provided as its first argument, recording a trace of what operators are used during the execution, and then converts those operators into ONNX equivalents. Because `torch.onnx.export` runs the model, we need to provide the function with an input tensor as its second argument, which can be random so long as it satisfies the shape and type of the model's input. As of Furiosa SDK v0.9, ONNX OpSet 13 is the most well-supported version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad941fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dummy input of the shape, (1, 3, 224, 224), of the model's input.\n",
    "dummy_input = (torch.randn(1, 3, 224, 224),)\n",
    "\n",
    "# Export the PyTorch model into an ONNX model.\n",
    "torch.onnx.export(\n",
    "    torch_model,  # PyTorch model to export\n",
    "    dummy_input,  # model input\n",
    "    \"resnet50.onnx\",  # where to save the exported ONNX model\n",
    "    opset_version=13,  # the ONNX OpSet version to export the model to\n",
    "    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "    input_names=[\"input\"],  # the ONNX model's input names\n",
    "    output_names=[\"output\"],  # the ONNX model's output names\n",
    ")\n",
    "\n",
    "# Load the exported ONNX model.\n",
    "onnx_model = onnx.load_model(\"resnet50.onnx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63f0021d",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6b07456",
   "metadata": {},
   "source": [
    "We will use subsets of the ImageNet dataset for calibration and validation. \n",
    "\n",
    "You need to download `ILSVRC2012_img_val.tar` and `ILSVRC2012_devkit_t12.tar.gz` externally and place them in the `imagenet` directory. Torchvision cannot download the ImageNet dataset automatically because it is no longer publicly accessible: https://github.com/pytorch/vision/pull/1457.\n",
    "\n",
    "Note that it may take several minutes to run this step for the first time because it involves decompressing the archive files. It will take much less time to complete subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "637a0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet = torchvision.datasets.ImageNet(\"imagenet\", split=\"val\", transform=preprocess)\n",
    "imagenet_for_calibrate = torchvision.datasets.ImageNet(\n",
    "    \"imagenet\", split=\"val\", transform=calbirate_preprocess\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a125399e",
   "metadata": {},
   "source": [
    "## Calibrate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce5a7038",
   "metadata": {},
   "source": [
    "For quick demonstration, a small number of samples randomly chosen from the ImageNet dataset is used for calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7244e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_dataset = torch.utils.data.Subset(\n",
    "    imagenet_for_calibrate, torch.randperm(len(imagenet_for_calibrate))[:100]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9f256d4",
   "metadata": {},
   "source": [
    "We call the `optimize_model` function to optimize onnx model, before calibration/quantization of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da4681da",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = optimize_model(onnx_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1462c219",
   "metadata": {},
   "source": [
    "We use Calibrator to calibrate the model with various CalibrationMethod (e.g. MIN_MAX_ASYM, ENTROPY_ASYM, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6989ad40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Calibrator in module furiosa.quantizer.calibrator:\n",
      "\n",
      "class Calibrator(builtins.object)\n",
      " |  Calibrator(model: Union[onnx.onnx_ml_pb2.ModelProto, bytes], calibration_method: furiosa.quantizer.calibrator.CalibrationMethod, *, percentage: float = 99.99)\n",
      " |  \n",
      " |  Calibrator.\n",
      " |  \n",
      " |  This collects the values of tensors in an ONNX model and computes\n",
      " |  their ranges.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model: Union[onnx.onnx_ml_pb2.ModelProto, bytes], calibration_method: furiosa.quantizer.calibrator.CalibrationMethod, *, percentage: float = 99.99)\n",
      " |      Args:\n",
      " |          model (onnx.ModelProto or bytes): An ONNX model to\n",
      " |              calibrate.\n",
      " |          calibration_method (CalibrationMethod): A calibration\n",
      " |              method.\n",
      " |          percentage (float): A percentage to use with percentile\n",
      " |              calibration. Defaults to 99.99 (i.e. 99.99%-percentile\n",
      " |              calibration).\n",
      " |  \n",
      " |  collect_data(self, calibration_dataset: Iterable[Sequence[numpy.ndarray]]) -> None\n",
      " |      Collect the values of tensors that will be used for range\n",
      " |      computation.\n",
      " |      \n",
      " |      This can be called multiple times.\n",
      " |      \n",
      " |      Args:\n",
      " |          calibration_dataset (Iterable[Sequence[numpy.ndarray]]):\n",
      " |              An object that provides input data for the model one at\n",
      " |              a time.\n",
      " |  \n",
      " |  compute_range(self, verbose: bool = False) -> Dict[str, Tuple[float, float]]\n",
      " |      Estimate the ranges of the tensors on the basis of the collected\n",
      " |      data.\n",
      " |      \n",
      " |      Args:\n",
      " |          verbose (bool): Whether to show a progress bar, Defaults to\n",
      " |              False.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Dict[str, Tuple[float, float]]: A dictionary that maps a\n",
      " |              tensor name to a tuple of the tensor's min and max.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Calibrator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f747bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CalibrationMethod in module furiosa.quantizer.calibrator:\n",
      "\n",
      "class CalibrationMethod(enum.IntEnum)\n",
      " |  CalibrationMethod(value, names=None, *, module=None, qualname=None, type=None, start=1)\n",
      " |  \n",
      " |  Calibration method.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      MIN_MAX_ASYM (CalibrationMethod):\n",
      " |          Min-max calibration (Asymmetric).\n",
      " |      MIN_MAX_SYM (CalibrationMethod):\n",
      " |          Min-max calibration (Symmetric).\n",
      " |      ENTROPY_ASYM (CalibrationMethod):\n",
      " |          Entropy calibration (Aymmetric).\n",
      " |      ENTROPY_SYM (CalibrationMethod):\n",
      " |          Entropy calibration (Symmetric).\n",
      " |      PERCENTILE_ASYM (CalibrationMethod):\n",
      " |          Percentile calibration (Asymmetric).\n",
      " |      PERCENTILE_SYM (CalibrationMethod):\n",
      " |          Percentile calibration (Symmetric).\n",
      " |      MSE_ASYM (CalibrationMethod):\n",
      " |          Mean squared error (MSE) calibration (Asymmetric).\n",
      " |      MSE_SYM (CalibrationMethod):\n",
      " |          Mean squared error (MSE) calibration (Symmetric).\n",
      " |      SQNR_ASYM (CalibrationMethod):\n",
      " |          Signal-to-quantization-noise ratio (SQNR) calibration\n",
      " |          (Asymmetric).\n",
      " |      SQNR_SYM (CalibrationMethod):\n",
      " |          Signal-to-quantization-noise ratio (SQNR) calibration\n",
      " |          (Symmetric).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CalibrationMethod\n",
      " |      enum.IntEnum\n",
      " |      builtins.int\n",
      " |      enum.Enum\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  ENTROPY_ASYM = <CalibrationMethod.ENTROPY_ASYM: 2>\n",
      " |  \n",
      " |  ENTROPY_SYM = <CalibrationMethod.ENTROPY_SYM: 3>\n",
      " |  \n",
      " |  MIN_MAX_ASYM = <CalibrationMethod.MIN_MAX_ASYM: 0>\n",
      " |  \n",
      " |  MIN_MAX_SYM = <CalibrationMethod.MIN_MAX_SYM: 1>\n",
      " |  \n",
      " |  MSE_ASYM = <CalibrationMethod.MSE_ASYM: 6>\n",
      " |  \n",
      " |  MSE_SYM = <CalibrationMethod.MSE_SYM: 7>\n",
      " |  \n",
      " |  PERCENTILE_ASYM = <CalibrationMethod.PERCENTILE_ASYM: 4>\n",
      " |  \n",
      " |  PERCENTILE_SYM = <CalibrationMethod.PERCENTILE_SYM: 5>\n",
      " |  \n",
      " |  SQNR_ASYM = <CalibrationMethod.SQNR_ASYM: 8>\n",
      " |  \n",
      " |  SQNR_SYM = <CalibrationMethod.SQNR_SYM: 9>\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from enum.Enum:\n",
      " |  \n",
      " |  name\n",
      " |      The name of the Enum member.\n",
      " |  \n",
      " |  value\n",
      " |      The value of the Enum member.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from enum.EnumMeta:\n",
      " |  \n",
      " |  __members__\n",
      " |      Returns a mapping of member name->value.\n",
      " |      \n",
      " |      This mapping lists all enum members, including aliases. Note that this\n",
      " |      is a read-only view of the internal mapping.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CalibrationMethod)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81a51835",
   "metadata": {},
   "source": [
    "Before the Calibrator actually computes the ranges, input data should be collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df9f8abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibration: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:06<00:00,  1.51images/s]\n"
     ]
    }
   ],
   "source": [
    "calibrator = Calibrator(onnx_model, CalibrationMethod.MIN_MAX_ASYM)\n",
    "\n",
    "for calibration_data, _ in tqdm.tqdm(\n",
    "    calibration_dataset, desc=\"Calibration\", unit=\"images\", mininterval=0.5\n",
    "):\n",
    "    cal_input = np.expand_dims(calibration_data.numpy(), axis=0)\n",
    "    calibrator.collect_data([[cal_input]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61d9e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = calibrator.compute_range()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52047fe2",
   "metadata": {},
   "source": [
    "## Use ModelEditor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3506183c",
   "metadata": {},
   "source": [
    "Now, we can use the ModelEditor to apply optimization to the input and output of the ONNX model. This optimization will transform them into a format that is efficient for our NPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "571b6d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "furiosa_editor = ModelEditor(onnx_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac127f33",
   "metadata": {},
   "source": [
    "We have defined a preprocess to handle the image data as u8 type. \n",
    "\n",
    "In order to use it in this way, we need to specify the input tensor of the model to use u8 type instead of f32 type. We can achieve this by setting `convert_input_type`(or `convert_output_type`) to enable using u8 instead of f32 for the corresponding input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a133bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use model input tensor which name is \"input\" as u8 type instead of f32 type\n",
    "furiosa_editor.convert_input_type(\"input\", TensorType.UINT8)\n",
    "# use model output tensor which name is \"output\" as i8 type instead of f32 type\n",
    "furiosa_editor.convert_output_type(\"output\", TensorType.INT8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "226a1453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ModelEditor in module furiosa.quantizer.editor:\n",
      "\n",
      "class ModelEditor(builtins.object)\n",
      " |  ModelEditor(model: onnx.onnx_ml_pb2.ModelProto)\n",
      " |  \n",
      " |  A utility class for manipulating ONNX models.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model: onnx.onnx_ml_pb2.ModelProto)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  convert_input_type(self, tensor_name: str, tensor_type: furiosa.quantizer.editor.TensorType) -> None\n",
      " |      Convert the element type of an input tensor named tensor_name to tensor_type.\n",
      " |      \n",
      " |      Args:\n",
      " |          tensor_name (str): The name of an input tensor to convert.\n",
      " |          tensor_type (TensorType): The desired element type.\n",
      " |  \n",
      " |  convert_output_type(self, tensor_name: str, tensor_type: furiosa.quantizer.editor.TensorType, tensor_range: Optional[Tuple[float, float]] = None) -> None\n",
      " |      Convert the element type of an output tensor named tensor_name to tensor_type.\n",
      " |      \n",
      " |      Args:\n",
      " |          tensor_name (str): The name of an output tensor to convert.\n",
      " |          tensor_type (TensorType): The desired element type.\n",
      " |          tensor_range (Optional[Tuple[float, float]]): A new min/max range of the output tensor.\n",
      " |              If it is None, the original range will be retained. Defaults to None.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ModelEditor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1523e086",
   "metadata": {},
   "source": [
    "## Quantize ONNX Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2df4392b",
   "metadata": {},
   "source": [
    "With the range computed, now we can quantize the model by calling `quantize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57a25a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"ranges.json\", \"w\") as f:\n",
    "    f.write(json.dumps(ranges, indent=4))\n",
    "with open(\"ranges.json\", \"r\") as f:\n",
    "    ranges = json.load(f)\n",
    "\n",
    "graph = quantize(onnx_model, ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "361a1045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function quantize in module furiosa.quantizer:\n",
      "\n",
      "quantize(model: Union[onnx.onnx_ml_pb2.ModelProto, bytes], tensor_name_to_range: Mapping[str, Sequence[float]]) -> bytes\n",
      "    Quantize an ONNX model on the basis of the range of its tensors.\n",
      "    \n",
      "    Args:\n",
      "        model (onnx.ModelProto or bytes): An ONNX model to quantize.\n",
      "        tensor_name_to_range (Mapping[str, Sequence[float]]):\n",
      "            A mapping from a tensor name to a 2-tuple (or list) of the\n",
      "            tensor's min and max.\n",
      "    \n",
      "    Returns:\n",
      "        bytes: A serialized ONNX model that incorporates quantization\n",
      "            information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(quantize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a960afe2",
   "metadata": {},
   "source": [
    "In case you want already have calibrated model once and have ranges info, you can save the ranges info inside a file and\n",
    "load it in order to skip calibration phase."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6616073d",
   "metadata": {},
   "source": [
    "## Run Inference with Quantized Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6793276f",
   "metadata": {},
   "source": [
    "For quick demonstration, we use randomly chosen 1000 samples from the ImageNet dataset for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94d92bef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2023-10-22T11:41:31.462682Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mfuriosa_rt_core::consts::envs\u001b[0m\u001b[2m:\u001b[0m NPU_DEVNAME will be deprecated. Use FURIOSA_DEVICES instead.\n",
      "\u001b[2m2023-10-22T11:41:31.462746Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_device::config::env\u001b[0m\u001b[2m:\u001b[0m Using config \"npu0pe0-1\" from environment variable \"NPU_DEVNAME\"\n",
      "\u001b[2m2023-10-22T11:41:31.467343Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m FuriosaRT (v0.10.2, rev: a45bb1a0b, built at: 2023-10-12T06:41:21Z) bootstrapping ...\n",
      "\u001b[2m2023-10-22T11:41:31.538585Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m Found furiosa-compiler (v0.10.1, rev: 8b00177, built at: 2023-10-12T06:26:59Z)\n",
      "\u001b[2m2023-10-22T11:41:31.538613Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m Found libhal (type: warboy, v0.11.0, rev: 43c901f built at: 2023-08-08T12:07:35Z)\n",
      "\u001b[2m2023-10-22T11:41:31.538622Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Runtime-0] detected 1 NPU device(s):\n",
      "\u001b[2m2023-10-22T11:41:31.553513Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m - [0] npu:0:0-1 (warboy-b0-2pe, 128dpes, firmware: 1.7.3290708996, f7b0f28)\n",
      "\u001b[2m2023-10-22T11:41:31.553747Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Runtime-0] started\n",
      "\u001b[2m2023-10-22T11:41:33.701207Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa::runtime\u001b[0m\u001b[2m:\u001b[0m Saving the compilation log into /root/.local/state/furiosa/logs/compiler-20231022204133-mat3ii.log\n",
      "\u001b[2m2023-10-22T11:41:33.701440Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Runtime-0] created Sess-eaa514b9 using npu:0:0-1\n",
      "\u001b[2m2023-10-22T11:41:33.713595Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Sess-eaa514b9] compiling the model (target: warboy-b0-2pe, 128dpes, size: 102.2 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[2m[1/6]\u001b[0m 🔍   Compiling from onnx to dfg\n",
      "Done in 1.158837s\n",
      "\u001b[1m\u001b[2m[2/6]\u001b[0m 🔍   Compiling from dfg to ldfg\n",
      "Done in 342.50912s\n",
      "\u001b[1m\u001b[2m[3/6]\u001b[0m 🔍   Compiling from ldfg to cdfg\n",
      "Done in 0.003204567s\n",
      "\u001b[1m\u001b[2m[4/6]\u001b[0m 🔍   Compiling from cdfg to gir\n",
      "Done in 0.028225036s\n",
      "\u001b[1m\u001b[2m[5/6]\u001b[0m 🔍   Compiling from gir to lir\n",
      "Done in 0.007152195s\n",
      "\u001b[1m\u001b[2m[6/6]\u001b[0m 🔍   Compiling from lir to enf\n",
      "Done in 0.16137625s\n",
      "✨  Finished in 343.86798s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2023-10-22T11:47:20.185652Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Sess-eaa514b9] the model compile is successful (took 346 secs)\n",
      "\u001b[2m2023-10-22T11:47:20.654257Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Runtime-0] created 1 NPU threads on npu:0:0-1 (DRAM: 180.0 kiB/16.0 GiB, SRAM: 31.7 MiB/128.0 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:14<00:00, 70.45images/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2023-10-22T11:47:35.096744Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Sess-eaa514b9] terminated\n",
      "\u001b[2m2023-10-22T11:47:35.102254Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::npu::raw\u001b[0m\u001b[2m:\u001b[0m NPU (npu:0:0-1) has been closed\n",
      "\u001b[2m2023-10-22T11:47:35.107646Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Runtime-0] stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = torch.utils.data.Subset(\n",
    "    imagenet, torch.randperm(len(imagenet))[:1000]\n",
    ")\n",
    "\n",
    "correct_predictions, total_predictions = 0, 0\n",
    "elapsed_time = 0\n",
    "with furiosa.runtime.sync.create_runner(graph) as runner:\n",
    "    for image, label in tqdm.tqdm(\n",
    "        validation_dataset, desc=\"Evaluation\", unit=\"images\", mininterval=0.5\n",
    "    ):\n",
    "        start = time.perf_counter_ns()\n",
    "        image = np.expand_dims(image.numpy(), axis=0)\n",
    "        outputs = runner.run(image)\n",
    "        elapsed_time += time.perf_counter_ns() - start\n",
    "\n",
    "        prediction = np.argmax(outputs[0], axis=1)[0]  # postprocessing\n",
    "        if prediction == label:\n",
    "            correct_predictions += 1\n",
    "        total_predictions += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2d16e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.200000%\n",
      "Average Latency: 2.026233351 ms\n"
     ]
    }
   ],
   "source": [
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Accuracy: {accuracy:%}\")\n",
    "\n",
    "latency = elapsed_time / total_predictions\n",
    "print(f\"Average Latency: {latency / 1_000_000} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e02ba53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
