{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b067241e",
   "metadata": {},
   "source": [
    "# How to Use Furiosa SDK from Start to Finish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5ecbcb",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use Furiosa SDK from start to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d1f6e7",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b143f2",
   "metadata": {},
   "source": [
    "The Furiosa SDK needs to have been installed. If not, it can be installed following instructions on https://furiosa-ai.github.io/docs/latest/ko/ (Korean) or https://furiosa-ai.github.io/docs/latest/en/ (English). The `torchvision` and `scipy` packages also need to be installed for this demonstration.\n",
    "\n",
    "```console\n",
    "$ pip install furiosa-sdk torchvision scipy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65b9439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import onnx\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import tqdm\n",
    "\n",
    "import furiosa.runtime.session\n",
    "from furiosa.optimizer import optimize_model\n",
    "from furiosa.quantizer import quantize, Calibrator, CalibrationMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318cb543",
   "metadata": {},
   "source": [
    "## Load PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26114a1c",
   "metadata": {},
   "source": [
    "As a running example, we employ the pre-trained ResNet-50 model from Torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c157524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = torchvision.models.resnet50(weights='DEFAULT')\n",
    "torch_model = torch_model.eval()  # Set the model to inference mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b228423",
   "metadata": {},
   "source": [
    "The ResNet50 model has been trained with the following preprocessing applied: https://pytorch.org/vision/stable/models.html We will use the same preprocessing for calibration and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23cbe19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc90e0e9",
   "metadata": {},
   "source": [
    "## Export PyTorch Model to ONNX Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995a571b",
   "metadata": {},
   "source": [
    "We call the `torch.onnx.export` function to export the PyTorch ResNet-50 model to an ONNX model. The function executes a PyTorch model provided as its first argument, recording a trace of what operators are used during the execution, and then converts those operators into ONNX equivalents. Because `torch.onnx.export` runs the model, we need to provide the function with an input tensor as its second argument, which can be random so long as it satisfies the shape and type of the model's input. As of Furiosa SDK v0.6, ONNX OpSet 12 is the most well-supported version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad941fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dummy input of the shape, (1, 3, 224, 224), of the model's input.\n",
    "dummy_input = (torch.randn(1, 3, 224, 224),)\n",
    "\n",
    "# Export the PyTorch model into an ONNX model.\n",
    "torch.onnx.export(\n",
    "    torch_model,  # PyTorch model to export\n",
    "    dummy_input,  # model input\n",
    "    \"resnet50.onnx\",  # where to save the exported ONNX model\n",
    "    opset_version=13,  # the ONNX OpSet version to export the model to\n",
    "    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "    input_names=[\"input\"],  # the ONNX model's input names\n",
    "    output_names=[\"output\"],  # the ONNX model's output names\n",
    ")\n",
    "\n",
    "# Load the exported ONNX model.\n",
    "onnx_model = onnx.load_model(\"resnet50.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0021d",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b07456",
   "metadata": {},
   "source": [
    "We will use subsets of the ImageNet dataset for calibration and validation. \n",
    "\n",
    "You need to download `ILSVRC2012_img_val.tar` and `ILSVRC2012_devkit_t12.tar.gz` externally and place them in the `imagenet` directory. Torchvision cannot download the ImageNet dataset automatically because it is no longer publicly accessible: https://github.com/pytorch/vision/pull/1457.\n",
    "\n",
    "Note that it may take several minutes to run this step for the first time because it involves decompressing the archive files. It will take much less time to complete subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "637a0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet = torchvision.datasets.ImageNet(\"imagenet\", split=\"val\", transform=preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a125399e",
   "metadata": {},
   "source": [
    "## Calibrate and Quantize ONNX Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5a7038",
   "metadata": {},
   "source": [
    "For quick demonstration, a small number of samples randomly chosen from the ImageNet dataset is used for calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7244e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_dataset = torch.utils.data.Subset(imagenet, torch.randperm(len(imagenet))[:100])\n",
    "calibration_dataloader = torch.utils.data.DataLoader(calibration_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f256d4",
   "metadata": {},
   "source": [
    "We call the `optimize_model` function to optimize onnx model, before calibration/quantization of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da4681da",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = optimize_model(onnx_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1462c219",
   "metadata": {},
   "source": [
    "We use Calibrator to calibrate the model with various CalibrationMethod (e.g. MIN_MAX_ASYM, ENTROPY_ASYM, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6989ad40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Calibrator in module furiosa.quantizer:\n",
      "\n",
      "class Calibrator(builtins.object)\n",
      " |  Calibrator(model: Union[onnx.onnx_ml_pb2.ModelProto, bytes], calibration_method: furiosa.quantizer.CalibrationMethod, *, percentage: float = 99.99)\n",
      " |  \n",
      " |  Calibrator.\n",
      " |  \n",
      " |  This collects the values of tensors in an ONNX model and computes\n",
      " |  their ranges.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model: Union[onnx.onnx_ml_pb2.ModelProto, bytes], calibration_method: furiosa.quantizer.CalibrationMethod, *, percentage: float = 99.99)\n",
      " |      Args:\n",
      " |          model (onnx.ModelProto or bytes): An ONNX model to\n",
      " |              calibrate.\n",
      " |          calibration_method (CalibrationMethod): A calibration\n",
      " |              method.\n",
      " |          percentage (float): A percentage to use with percentile\n",
      " |              calibration. Defaults to 99.99 (i.e. 99.99%-percentile\n",
      " |              calibration).\n",
      " |  \n",
      " |  collect_data(self, calibration_dataset: Iterable[Sequence[numpy.ndarray]]) -> None\n",
      " |      Collect the values of tensors that will be used for range\n",
      " |      computation.\n",
      " |      \n",
      " |      This can be called multiple times.\n",
      " |      \n",
      " |      Args:\n",
      " |          calibration_dataset (Iterable[Sequence[numpy.ndarray]]):\n",
      " |              An object that provides input data for the model one at\n",
      " |              a time.\n",
      " |  \n",
      " |  compute_range(self, verbose: bool = False) -> Dict[str, Tuple[float, float]]\n",
      " |      Estimate the ranges of the tensors on the basis of the collected\n",
      " |      data.\n",
      " |      \n",
      " |      Args:\n",
      " |          verbose (bool): Whether to show a progress bar, Defaults to\n",
      " |              False.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Dict[str, Tuple[float, float]]: A dictionary that maps a\n",
      " |              tensor name to a tuple of the tensor's min and max.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Calibrator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f747bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CalibrationMethod in module furiosa.quantizer:\n",
      "\n",
      "class CalibrationMethod(enum.IntEnum)\n",
      " |  CalibrationMethod(value, names=None, *, module=None, qualname=None, type=None, start=1)\n",
      " |  \n",
      " |  Calibration method.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      MIN_MAX_ASYM (CalibrationMethod):\n",
      " |          Min-max calibration (Asymmetric).\n",
      " |      MIN_MAX_SYM (CalibrationMethod):\n",
      " |          Min-max calibration (Symmetric).\n",
      " |      ENTROPY_ASYM (CalibrationMethod):\n",
      " |          Entropy calibration (Aymmetric).\n",
      " |      ENTROPY_SYM (CalibrationMethod):\n",
      " |          Entropy calibration (Symmetric).\n",
      " |      PERCENTILE_ASYM (CalibrationMethod):\n",
      " |          Percentile calibration (Asymmetric).\n",
      " |      PERCENTILE_SYM (CalibrationMethod):\n",
      " |          Percentile calibration (Symmetric).\n",
      " |      MSE_ASYM (CalibrationMethod):\n",
      " |          Mean squared error (MSE) calibration (Asymmetric).\n",
      " |      MSE_SYM (CalibrationMethod):\n",
      " |          Mean squared error (MSE) calibration (Symmetric).\n",
      " |      SQNR_ASYM (CalibrationMethod):\n",
      " |          Signal-to-quantization-noise ratio (SQNR) calibration\n",
      " |          (Asymmetric).\n",
      " |      SQNR_SYM (CalibrationMethod):\n",
      " |          Signal-to-quantization-noise ratio (SQNR) calibration\n",
      " |          (Symmetric).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CalibrationMethod\n",
      " |      enum.IntEnum\n",
      " |      builtins.int\n",
      " |      enum.Enum\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  ENTROPY_ASYM = <CalibrationMethod.ENTROPY_ASYM: 2>\n",
      " |  \n",
      " |  ENTROPY_SYM = <CalibrationMethod.ENTROPY_SYM: 3>\n",
      " |  \n",
      " |  MIN_MAX_ASYM = <CalibrationMethod.MIN_MAX_ASYM: 0>\n",
      " |  \n",
      " |  MIN_MAX_SYM = <CalibrationMethod.MIN_MAX_SYM: 1>\n",
      " |  \n",
      " |  MSE_ASYM = <CalibrationMethod.MSE_ASYM: 6>\n",
      " |  \n",
      " |  MSE_SYM = <CalibrationMethod.MSE_SYM: 7>\n",
      " |  \n",
      " |  PERCENTILE_ASYM = <CalibrationMethod.PERCENTILE_ASYM: 4>\n",
      " |  \n",
      " |  PERCENTILE_SYM = <CalibrationMethod.PERCENTILE_SYM: 5>\n",
      " |  \n",
      " |  SQNR_ASYM = <CalibrationMethod.SQNR_ASYM: 8>\n",
      " |  \n",
      " |  SQNR_SYM = <CalibrationMethod.SQNR_SYM: 9>\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from enum.Enum:\n",
      " |  \n",
      " |  name\n",
      " |      The name of the Enum member.\n",
      " |  \n",
      " |  value\n",
      " |      The value of the Enum member.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from enum.EnumMeta:\n",
      " |  \n",
      " |  __members__\n",
      " |      Returns a mapping of member name->value.\n",
      " |      \n",
      " |      This mapping lists all enum members, including aliases. Note that this\n",
      " |      is a read-only view of the internal mapping.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CalibrationMethod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a51835",
   "metadata": {},
   "source": [
    "Before the Calibrator actually computes the ranges, input data should be collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df9f8abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibration: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:13<00:00,  7.41images/s]\n"
     ]
    }
   ],
   "source": [
    "calibrator = Calibrator(onnx_model, CalibrationMethod.MIN_MAX_ASYM)\n",
    "\n",
    "for calibration_data, _ in tqdm.tqdm(calibration_dataloader, desc=\"Calibration\", unit=\"images\", mininterval=0.5):\n",
    "    calibrator.collect_data([[calibration_data.numpy()]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61d9e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = calibrator.compute_range()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86b4ce95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/layer2/layer2.2/relu_1/Relu_output_0': (0.0, 38.29145050048828),\n",
       " '/layer1/layer1.0/conv1/Conv_output_0': (-47.37602996826172,\n",
       "  17.679630279541016),\n",
       " '/layer4/layer4.2/Add_output_0': (-36.31756591796875, 63.91482162475586),\n",
       " '/layer3/layer3.5/conv3/Conv_output_0': (-32.128360748291016,\n",
       "  28.7648983001709),\n",
       " '/layer3/layer3.2/conv2/Conv_output_0': (-30.317304611206055,\n",
       "  24.274999618530273),\n",
       " '/layer1/layer1.0/relu_2/Relu_output_0': (0.0, 23.12796401977539),\n",
       " '/layer2/layer2.2/relu/Relu_output_0': (0.0, 84.18418884277344),\n",
       " '/layer1/layer1.1/conv1/Conv_output_0': (-74.73363494873047,\n",
       "  22.021080017089844),\n",
       " '/layer2/layer2.0/Add_output_0': (-58.33538818359375, 89.58488464355469),\n",
       " '/layer3/layer3.1/conv1/Conv_output_0': (-46.887962341308594,\n",
       "  46.5382194519043),\n",
       " '/layer4/layer4.1/conv2/Conv_output_0': (-35.85947799682617,\n",
       "  20.508670806884766),\n",
       " '/layer3/layer3.1/conv3/Conv_output_0': (-121.7065200805664,\n",
       "  56.117897033691406),\n",
       " 'output': (-2.7943978309631348, 9.053367614746094),\n",
       " '/layer3/layer3.4/relu/Relu_output_0': (0.0, 65.08959197998047),\n",
       " '/layer3/layer3.4/conv2/Conv_output_0': (-52.048004150390625,\n",
       "  37.04460525512695),\n",
       " '/layer3/layer3.2/Add_output_0': (-60.503379821777344, 52.92853927612305),\n",
       " '/layer4/layer4.1/relu/Relu_output_0': (0.0, 36.7391471862793),\n",
       " '/layer1/layer1.0/conv2/Conv_output_0': (-21.426658630371094,\n",
       "  24.85123062133789),\n",
       " '/layer1/layer1.1/Add_output_0': (-65.26943969726562, 23.946752548217773),\n",
       " '/layer3/layer3.0/relu_2/Relu_output_0': (0.0, 96.35240173339844),\n",
       " '/layer3/layer3.5/conv2/Conv_output_0': (-24.299365997314453,\n",
       "  22.78024673461914),\n",
       " '/layer4/layer4.2/conv3/Conv_output_0': (-57.35306930541992,\n",
       "  37.9136848449707),\n",
       " '/layer3/layer3.2/relu/Relu_output_0': (0.0, 26.16423988342285),\n",
       " '/layer4/layer4.1/relu_1/Relu_output_0': (0.0, 20.508670806884766),\n",
       " '/layer3/layer3.0/relu/Relu_output_0': (0.0, 49.200469970703125),\n",
       " '/layer1/layer1.2/conv1/Conv_output_0': (-33.250091552734375,\n",
       "  20.909461975097656),\n",
       " '/layer4/layer4.1/Add_output_0': (-37.57780075073242, 81.09672546386719),\n",
       " '/layer3/layer3.4/conv1/Conv_output_0': (-19.45911979675293,\n",
       "  65.08959197998047),\n",
       " '/layer3/layer3.1/relu_1/Relu_output_0': (0.0, 49.52044677734375),\n",
       " '/layer2/layer2.3/Add_output_0': (-89.75640106201172, 155.3320770263672),\n",
       " '/layer3/layer3.0/conv1/Conv_output_0': (-55.66450500488281,\n",
       "  49.200469970703125),\n",
       " '/layer3/layer3.0/downsample/downsample.0/Conv_output_0': (-51.301673889160156,\n",
       "  77.0583267211914),\n",
       " '/layer3/layer3.3/relu_2/Relu_output_0': (0.0, 72.32341766357422),\n",
       " '/layer2/layer2.1/conv3/Conv_output_0': (-108.82154846191406,\n",
       "  107.69303894042969),\n",
       " '/layer2/layer2.1/relu_1/Relu_output_0': (0.0, 42.81747055053711),\n",
       " '/layer2/layer2.0/relu_2/Relu_output_0': (0.0, 89.58488464355469),\n",
       " '/layer2/layer2.1/Add_output_0': (-38.53976058959961, 173.697021484375),\n",
       " '/layer3/layer3.1/conv2/Conv_output_0': (-76.44017791748047,\n",
       "  49.52044677734375),\n",
       " '/layer2/layer2.3/conv1/Conv_output_0': (-79.04878997802734,\n",
       "  83.60856628417969),\n",
       " '/layer1/layer1.1/relu_1/Relu_output_0': (0.0, 23.75821876525879),\n",
       " '/layer4/layer4.0/downsample/downsample.0/Conv_output_0': (-46.36024475097656,\n",
       "  106.66339111328125),\n",
       " '/layer4/layer4.1/conv3/Conv_output_0': (-49.381752014160156,\n",
       "  59.63408660888672),\n",
       " '/maxpool/MaxPool_output_0': (0.0, 39.804161071777344),\n",
       " '/layer2/layer2.2/conv1/Conv_output_0': (-113.65699768066406,\n",
       "  84.18418884277344),\n",
       " '/layer4/layer4.0/conv3/Conv_output_0': (-36.04325866699219,\n",
       "  51.497406005859375),\n",
       " '/layer1/layer1.0/downsample/downsample.0/Conv_output_0': (-58.16147994995117,\n",
       "  29.520713806152344),\n",
       " '/layer2/layer2.0/downsample/downsample.0/Conv_output_0': (-35.79826354980469,\n",
       "  23.11131477355957),\n",
       " '/layer2/layer2.3/relu_1/Relu_output_0': (0.0, 37.12760925292969),\n",
       " '/layer1/layer1.0/relu/Relu_output_0': (0.0, 17.679630279541016),\n",
       " '/layer3/layer3.0/conv2/Conv_output_0': (-24.559221267700195,\n",
       "  18.053916931152344),\n",
       " '/layer3/layer3.5/relu_2/Relu_output_0': (0.0, 49.55634307861328),\n",
       " '/layer1/layer1.0/Add_output_0': (-53.73468780517578, 23.12796401977539),\n",
       " '/layer2/layer2.3/relu_2/Relu_output_0': (0.0, 155.3320770263672),\n",
       " 'input': (-2.1179039478302, 2.640000104904175),\n",
       " '/layer1/layer1.0/conv3/Conv_output_0': (-19.165258407592773,\n",
       "  22.002471923828125),\n",
       " '/layer3/layer3.5/relu/Relu_output_0': (0.0, 23.45096778869629),\n",
       " '/layer1/layer1.1/relu_2/Relu_output_0': (0.0, 23.946752548217773),\n",
       " '/layer3/layer3.2/relu_1/Relu_output_0': (0.0, 24.274999618530273),\n",
       " '/layer4/layer4.0/relu_2/Relu_output_0': (0.0, 99.46241760253906),\n",
       " '/avgpool/GlobalAveragePool_output_0': (0.0, 9.533331871032715),\n",
       " '/layer1/layer1.2/conv2/Conv_output_0': (-27.306610107421875,\n",
       "  47.48180389404297),\n",
       " '/layer2/layer2.0/relu/Relu_output_0': (0.0, 45.972957611083984),\n",
       " '/layer2/layer2.3/conv2/Conv_output_0': (-94.07167053222656,\n",
       "  37.12760925292969),\n",
       " '/layer3/layer3.1/relu_2/Relu_output_0': (0.0, 65.5597915649414),\n",
       " '/layer2/layer2.2/Add_output_0': (-63.81571578979492, 175.71481323242188),\n",
       " '/layer3/layer3.3/conv1/Conv_output_0': (-21.644210815429688,\n",
       "  41.70574188232422),\n",
       " '/layer3/layer3.5/conv1/Conv_output_0': (-20.533966064453125,\n",
       "  23.45096778869629),\n",
       " '/layer2/layer2.0/conv2/Conv_output_0': (-21.877931594848633,\n",
       "  56.7411003112793),\n",
       " '/layer1/layer1.2/relu_2/Relu_output_0': (0.0, 124.7042465209961),\n",
       " '/layer3/layer3.3/relu/Relu_output_0': (0.0, 41.70574188232422),\n",
       " '/layer4/layer4.1/relu_2/Relu_output_0': (0.0, 81.09672546386719),\n",
       " '/layer2/layer2.1/conv1/Conv_output_0': (-75.87328338623047,\n",
       "  92.16399383544922),\n",
       " '/relu/Relu_output_0': (0.0, 39.804161071777344),\n",
       " '/layer3/layer3.0/Add_output_0': (-47.31008529663086, 96.35240173339844),\n",
       " '/layer3/layer3.1/Add_output_0': (-91.99053955078125, 65.5597915649414),\n",
       " '/layer1/layer1.1/conv2/Conv_output_0': (-28.8778018951416,\n",
       "  23.75821876525879),\n",
       " '/layer3/layer3.2/relu_2/Relu_output_0': (0.0, 52.92853927612305),\n",
       " '/layer3/layer3.0/conv3/Conv_output_0': (-24.953908920288086,\n",
       "  36.148006439208984),\n",
       " '/layer2/layer2.0/conv3/Conv_output_0': (-37.8198127746582,\n",
       "  79.56212615966797),\n",
       " '/layer3/layer3.3/conv3/Conv_output_0': (-69.72999572753906,\n",
       "  72.1409683227539),\n",
       " '/layer3/layer3.4/relu_2/Relu_output_0': (0.0, 56.37242126464844),\n",
       " '/layer4/layer4.0/conv2/Conv_output_0': (-23.641077041625977,\n",
       "  19.21565055847168),\n",
       " '/layer3/layer3.2/conv1/Conv_output_0': (-24.000741958618164,\n",
       "  26.16423988342285),\n",
       " 'output_fused': (-2.7943978309631348, 9.053367614746094),\n",
       " '/layer3/layer3.4/Add_output_0': (-63.695865631103516, 56.37242126464844),\n",
       " '/layer3/layer3.3/conv2/Conv_output_0': (-27.91364860534668,\n",
       "  47.85311508178711),\n",
       " '/layer1/layer1.0/relu_1/Relu_output_0': (0.0, 24.85123062133789),\n",
       " '/layer1/layer1.2/Add_output_0': (-45.167667388916016, 124.7042465209961),\n",
       " '/layer3/layer3.0/relu_1/Relu_output_0': (0.0, 18.053916931152344),\n",
       " '/layer2/layer2.1/relu/Relu_output_0': (0.0, 92.16399383544922),\n",
       " '/layer4/layer4.2/relu/Relu_output_0': (0.0, 34.058589935302734),\n",
       " '/layer1/layer1.2/relu/Relu_output_0': (0.0, 20.909461975097656),\n",
       " '/layer2/layer2.2/conv2/Conv_output_0': (-75.01013946533203,\n",
       "  38.29145050048828),\n",
       " '/layer2/layer2.1/conv2/Conv_output_0': (-31.78311538696289,\n",
       "  42.81747055053711),\n",
       " '/layer3/layer3.3/relu_1/Relu_output_0': (0.0, 47.85311508178711),\n",
       " '/layer1/layer1.1/conv3/Conv_output_0': (-65.26943969726562,\n",
       "  19.465055465698242),\n",
       " '/layer2/layer2.3/conv3/Conv_output_0': (-89.75640106201172,\n",
       "  27.786272048950195),\n",
       " '/layer3/layer3.4/conv3/Conv_output_0': (-63.695865631103516,\n",
       "  55.75996398925781),\n",
       " '/layer4/layer4.0/relu_1/Relu_output_0': (0.0, 19.21565055847168),\n",
       " '/layer4/layer4.2/relu_1/Relu_output_0': (0.0, 16.933961868286133),\n",
       " '/layer4/layer4.0/relu/Relu_output_0': (0.0, 25.85301399230957),\n",
       " '/layer4/layer4.2/conv1/Conv_output_0': (-47.49589920043945,\n",
       "  34.058589935302734),\n",
       " '/layer2/layer2.0/conv1/Conv_output_0': (-48.70290756225586,\n",
       "  45.972957611083984),\n",
       " '/layer2/layer2.0/relu_1/Relu_output_0': (0.0, 56.7411003112793),\n",
       " '/layer4/layer4.1/conv1/Conv_output_0': (-33.417694091796875,\n",
       "  36.7391471862793),\n",
       " '/layer3/layer3.3/Add_output_0': (-69.72999572753906, 72.32341766357422),\n",
       " '/layer2/layer2.3/relu/Relu_output_0': (0.0, 83.60856628417969),\n",
       " '/layer4/layer4.2/conv2/Conv_output_0': (-40.697967529296875,\n",
       "  16.933961868286133),\n",
       " '/layer1/layer1.2/conv3/Conv_output_0': (-45.167667388916016,\n",
       "  124.7042465209961),\n",
       " '/layer4/layer4.0/conv1/Conv_output_0': (-20.641887664794922,\n",
       "  25.85301399230957),\n",
       " '/layer4/layer4.0/Add_output_0': (-49.53571701049805, 99.46241760253906),\n",
       " '/layer3/layer3.5/relu_1/Relu_output_0': (0.0, 22.78024673461914),\n",
       " '/layer3/layer3.5/Add_output_0': (-28.532644271850586, 49.55634307861328),\n",
       " '/layer3/layer3.4/relu_1/Relu_output_0': (0.0, 37.04460525512695),\n",
       " '/conv1/Conv_output_0': (-35.18403244018555, 39.804161071777344),\n",
       " '/layer1/layer1.2/relu_1/Relu_output_0': (0.0, 47.48180389404297),\n",
       " '/layer2/layer2.2/conv3/Conv_output_0': (-63.81571578979492,\n",
       "  27.038991928100586),\n",
       " '/layer2/layer2.2/relu_2/Relu_output_0': (0.0, 175.71481323242188),\n",
       " '/layer3/layer3.2/conv3/Conv_output_0': (-60.503379821777344,\n",
       "  24.453542709350586),\n",
       " '/layer3/layer3.1/relu/Relu_output_0': (0.0, 46.5382194519043),\n",
       " '/layer4/layer4.2/relu_2/Relu_output_0': (0.0, 63.91482162475586),\n",
       " '/layer2/layer2.1/relu_2/Relu_output_0': (0.0, 173.697021484375),\n",
       " '/layer1/layer1.1/relu/Relu_output_0': (0.0, 22.021080017089844)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df4392b",
   "metadata": {},
   "source": [
    "With the range computed, now we can quantize the model by calling `quantize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97761424",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = quantize(onnx_model, ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "361a1045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function quantize in module furiosa.quantizer:\n",
      "\n",
      "quantize(model: Union[onnx.onnx_ml_pb2.ModelProto, bytes], tensor_name_to_range: Mapping[str, Sequence[float]], *, with_quantize: bool = True) -> Graph\n",
      "    Quantize an ONNX model on the basis of the range of its tensors.\n",
      "    \n",
      "    Args:\n",
      "        model (onnx.ModelProto or bytes): An ONNX model to quantize.\n",
      "        tensor_name_to_range (Mapping[str, Sequence[float]]):\n",
      "            A mapping from a tensor name to a 2-tuple (or list) of the\n",
      "            tensor's min and max.\n",
      "        with_quantize (bool): Whether to put a Quantize operator at the\n",
      "            beginning of the resulting model. Defaults to True.\n",
      "    \n",
      "    Returns:\n",
      "        Graph: An intermediate representation (IR) of the quantized\n",
      "            model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(quantize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a960afe2",
   "metadata": {},
   "source": [
    "In case you want already have calibrated model once and have ranges info, you can save the ranges info inside a file and\n",
    "load it in order to skip calibration phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57a25a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"ranges.json\", \"w\") as f:\n",
    "    f.write(json.dumps(ranges, indent=4))\n",
    "with open(\"ranges.json\", \"r\") as f:\n",
    "    ranges = json.load(f)\n",
    "\n",
    "graph = quantize(onnx_model, ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6616073d",
   "metadata": {},
   "source": [
    "## Run Inference with Quantized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6793276f",
   "metadata": {},
   "source": [
    "For quick demonstration, we use randomly chosen 1000 samples from the ImageNet dataset for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94d92bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the compilation log into /root/.local/state/furiosa/logs/compile-20230202181132-i7iirw.log\n",
      "Using furiosa-compiler 0.9.0-dev (rev: ada3c6888 built at 2023-02-01T23:44:29Z)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2023-02-02T09:11:32.142082Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mnux::npu\u001b[0m\u001b[2m:\u001b[0m Npu (npu0pe0-1) is being initialized\n",
      "\u001b[2m2023-02-02T09:11:32.144634Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mnux\u001b[0m\u001b[2m:\u001b[0m NuxInner create with pes: [PeId(0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[2m[1/5]\u001b[0m 🔍   Compiling from dfg to ldfg\n",
      "Done in 70.44142s\n",
      "\u001b[1m\u001b[2m[2/5]\u001b[0m 🔍   Compiling from ldfg to cdfg\n",
      "Done in 0.002618423s\n",
      "\u001b[1m\u001b[2m[3/5]\u001b[0m 🔍   Compiling from cdfg to gir\n",
      "Done in 0.026109615s\n",
      "\u001b[1m\u001b[2m[4/5]\u001b[0m 🔍   Compiling from gir to lir\n",
      "Done in 0.007125562s\n",
      "\u001b[1m\u001b[2m[5/5]\u001b[0m 🔍   Compiling from lir to enf\n",
      "Done in 0.073303185s\n",
      "✨  Finished in 70.551125s\n",
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:07<00:00, 127.14images/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2023-02-02T09:12:53.081743Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mnux::npu\u001b[0m\u001b[2m:\u001b[0m NPU (npu0pe0-1) has been destroyed\n",
      "\u001b[2m2023-02-02T09:12:53.084427Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mnux::capi\u001b[0m\u001b[2m:\u001b[0m session has been destroyed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = torch.utils.data.Subset(imagenet, torch.randperm(len(imagenet))[:1000])\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=1)\n",
    "\n",
    "correct_predictions, total_predictions = 0, 0\n",
    "elapsed_time = 0\n",
    "with furiosa.runtime.session.create(graph) as session:\n",
    "    for image, label in tqdm.tqdm(validation_dataloader, desc=\"Evaluation\", unit=\"images\", mininterval=0.5):\n",
    "        image = image.numpy()\n",
    "        start = time.perf_counter_ns()\n",
    "        outputs = session.run(image)\n",
    "        elapsed_time += time.perf_counter_ns() - start\n",
    "        \n",
    "        prediction = np.argmax(outputs[0].numpy(), axis=1)  # postprocessing  \n",
    "        if prediction == label.numpy():\n",
    "            correct_predictions += 1\n",
    "        total_predictions += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2d16e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.800000%\n",
      "Average Latency: 2.6027747960000003 ms\n"
     ]
    }
   ],
   "source": [
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Accuracy: {accuracy:%}\")\n",
    "\n",
    "latency = elapsed_time / total_predictions\n",
    "print(f\"Average Latency: {latency / 1_000_000} ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
