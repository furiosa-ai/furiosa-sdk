{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b067241e",
   "metadata": {},
   "source": [
    "# How to Use Furiosa SDK from Start to Finish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5ecbcb",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use Furiosa SDK from start to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d1f6e7",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b143f2",
   "metadata": {},
   "source": [
    "The Furiosa SDK needs to have been installed. If not, it can be installed following instructions on https://furiosa-ai.github.io/docs/latest/ko/ (Korean) or https://furiosa-ai.github.io/docs/latest/en/ (English). The `torchvision` and `scipy` packages also need to be installed for this demonstration.\n",
    "\n",
    "```console\n",
    "$ pip install 'furiosa-sdk[quantizer]' torchvision scipy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a65b9439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libnpu.so --- v2.0, built @ e328545\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import onnx\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import tqdm\n",
    "\n",
    "import furiosa.runtime.session\n",
    "from furiosa.quantizer.frontend.onnx import post_training_quantize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318cb543",
   "metadata": {},
   "source": [
    "## Load PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26114a1c",
   "metadata": {},
   "source": [
    "As a running example, we employ the pre-trained ResNet-50 model from Torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c157524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = torchvision.models.resnet50(pretrained=True)\n",
    "torch_model = torch_model.eval()  # Set the model to inference mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b228423",
   "metadata": {},
   "source": [
    "The ResNet50 model has been trained with the following preprocessing applied: https://pytorch.org/vision/stable/models.html We will use the same preprocessing for calibration and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23cbe19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc90e0e9",
   "metadata": {},
   "source": [
    "## Export PyTorch Model to ONNX Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eac6ea",
   "metadata": {},
   "source": [
    "We call the `torch.onnx.export` function to export the PyTorch ResNet-50 model to an ONNX model. The function executes a PyTorch model provided as its first argument, recording a trace of what operators are used during the execution, and then converts those operators into ONNX equivalents. Because `torch.onnx.export` runs the model, we need to provide the function with an input tensor as its second argument, which can be random so long as it satisfies the shape and type of the model's input. As of Furiosa SDK v0.6, ONNX OpSet 12 is the most well-supported version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad941fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dummy input of the shape, (1, 3, 224, 224), of the model's input.\n",
    "dummy_input = (torch.randn(1, 3, 224, 224),)\n",
    "\n",
    "# Export the PyTorch model into an ONNX model.\n",
    "torch.onnx.export(\n",
    "    torch_model,  # PyTorch model to export\n",
    "    dummy_input,  # model input\n",
    "    \"resnet50.onnx\",  # where to save the exported ONNX model\n",
    "    opset_version=12,  # the ONNX OpSet version to export the model to\n",
    "    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "    input_names=[\"input\"],  # the ONNX model's input names\n",
    "    output_names=[\"output\"],  # the ONNX model's output names\n",
    ")\n",
    "\n",
    "# Load the exported ONNX model.\n",
    "onnx_model = onnx.load_model(\"resnet50.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0021d",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b07456",
   "metadata": {},
   "source": [
    "We will use subsets of the ImageNet dataset for calibration and validation. \n",
    "\n",
    "You need to download `ILSVRC2012_img_val.tar` and `ILSVRC2012_devkit_t12.tar.gz` externally and place them in the `imagenet` directory. Torchvision cannot download the ImageNet dataset automatically because it is no longer publicly accessible: https://github.com/pytorch/vision/pull/1457.\n",
    "\n",
    "Note that it may take several minutes to run this step for the first time because it involves decompressing the archive files. It will take much less time to complete subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "637a0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet = torchvision.datasets.ImageNet(\"imagenet\", split=\"val\", transform=preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95001cf",
   "metadata": {},
   "source": [
    "## Calibrate and Quantize ONNX Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f256d4",
   "metadata": {},
   "source": [
    "We call the `furiosa.quantizer.frontend.onnx.post_training_quantize` function to calibrate and quantize the ONNX model at one fell swoop. For quick demonstration, a small number of samples randomly chosen from the ImageNet dataset is used for calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f747bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibration: 100it [00:22,  4.48it/s]\n",
      "Quantization: 100%|███████████████████████████████████████████████| 122/122 [00:02<00:00, 50.59it/s]\n"
     ]
    }
   ],
   "source": [
    "calibration_dataset = torch.utils.data.Subset(imagenet, torch.randperm(len(imagenet))[:100])\n",
    "calibration_dataloader = torch.utils.data.DataLoader(calibration_dataset, batch_size=1)\n",
    "\n",
    "onnx_model_quantized = post_training_quantize(\n",
    "    onnx_model,\n",
    "    ({\"input\": image.numpy()} for image, _ in calibration_dataloader),\n",
    ")\n",
    "onnx.save_model(onnx_model_quantized, \"resnet50_quantized.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6616073d",
   "metadata": {},
   "source": [
    "## Run Inference with Quantized ONNX Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6793276f",
   "metadata": {},
   "source": [
    "For quick demonstration, we use randomly chosen 1000 samples from the ImageNet dataset for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94d92bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the compilation log into /root/.local/state/furiosa/logs/compile-20220325011645-slh1k0.log\n",
      "Using furiosa-compiler 0.6.1 (rev: 6edff7711 built at 2022-03-22 18:51:51)\n",
      "\u001b[2m2022-03-25T01:16:45.038999Z\u001b[0m \u001b[32m INFO\u001b[0m Npu (npu6pe0-1) is being initialized\n",
      "\u001b[2m2022-03-25T01:16:45.039809Z\u001b[0m \u001b[32m INFO\u001b[0m NuxInner create with pes: [PeId(0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/6] 🔍   Compiling from onnx to dfg\n",
      "Done in 0.040535457s\n",
      "[2/6] 🔍   Compiling from dfg to ldfg\n",
      "Done in 341.40164s\n",
      "[3/6] 🔍   Compiling from ldfg to cdfg\n",
      "Done in 0.001958493s\n",
      "[4/6] 🔍   Compiling from cdfg to gir\n",
      "Done in 0.022858769s\n",
      "[5/6] 🔍   Compiling from gir to lir\n",
      "Done in 0.005438614s\n",
      "[6/6] 🔍   Compiling from lir to enf\n",
      "Done in 0.064078905s\n",
      "✨  Finished in 341.53796s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2022-03-25T01:22:26.606689Z\u001b[0m \u001b[32m INFO\u001b[0m [Profiler] Program binary notification has been arrived. Cleanup current profile queue data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|███████████████████████████████████████████| 1000/1000 [00:22<00:00, 44.88images/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2022-03-25T01:22:49.489470Z\u001b[0m \u001b[32m INFO\u001b[0m [Profiler] Received a termination signal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2022-03-25T01:22:49.490451Z\u001b[0m \u001b[32m INFO\u001b[0m session has been destroyed\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = torch.utils.data.Subset(imagenet, torch.randperm(len(imagenet))[:1000])\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=1)\n",
    "\n",
    "correct_predictions, total_predictions = 0, 0\n",
    "elapsed_time = 0\n",
    "with furiosa.runtime.session.create(\"resnet50_quantized.onnx\") as session:\n",
    "    for image, label in tqdm.tqdm(validation_dataloader, desc=\"Evaluation\", unit=\"images\", mininterval=0.5):\n",
    "        image = image.numpy()\n",
    "        start = time.perf_counter_ns()\n",
    "        outputs = session.run(image)\n",
    "        elapsed_time += time.perf_counter_ns() - start\n",
    "        \n",
    "        prediction = np.argmax(outputs[0].numpy(), axis=1)  # postprocessing  \n",
    "        if prediction == label.numpy():\n",
    "            correct_predictions += 1\n",
    "        total_predictions += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2d16e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.200000%\n",
      "Average Latency: 8.689881424000001 ms\n"
     ]
    }
   ],
   "source": [
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Accuracy: {accuracy:%}\")\n",
    "\n",
    "latency = elapsed_time / total_predictions\n",
    "print(f\"Average Latency: {latency / 1_000_000} ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
