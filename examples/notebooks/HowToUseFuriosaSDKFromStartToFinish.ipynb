{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b067241e",
   "metadata": {},
   "source": [
    "# How to Use Furiosa SDK from Start to Finish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5ecbcb",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use Furiosa SDK from start to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d1f6e7",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b143f2",
   "metadata": {},
   "source": [
    "The Furiosa SDK needs to have been installed. If not, it can be installed following instructions on https://furiosa-ai.github.io/docs/latest/ko/ (Korean) or https://furiosa-ai.github.io/docs/latest/en/ (English). The `torchvision` and `scipy` packages also need to be installed for this demonstration.\n",
    "\n",
    "```console\n",
    "$ pip install furiosa-sdk torchvision scipy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a65b9439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libfuriosa_hal.so --- v2.0, built @ 5423ba8\n",
      "libfuriosa_hal.so --- v2.0, built @ 5423ba8\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import onnx\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import tqdm\n",
    "\n",
    "import furiosa.runtime.session\n",
    "from furiosa.optimizer import optimize_model\n",
    "from furiosa.quantizer import post_training_quantize, quantize, Calibrator, CalibrationMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318cb543",
   "metadata": {},
   "source": [
    "## Load PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26114a1c",
   "metadata": {},
   "source": [
    "As a running example, we employ the pre-trained ResNet-50 model from Torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c157524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = torchvision.models.resnet50(weights='DEFAULT')\n",
    "torch_model = torch_model.eval()  # Set the model to inference mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b228423",
   "metadata": {},
   "source": [
    "The ResNet50 model has been trained with the following preprocessing applied: https://pytorch.org/vision/stable/models.html We will use the same preprocessing for calibration and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23cbe19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc90e0e9",
   "metadata": {},
   "source": [
    "## Export PyTorch Model to ONNX Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995a571b",
   "metadata": {},
   "source": [
    "We call the `torch.onnx.export` function to export the PyTorch ResNet-50 model to an ONNX model. The function executes a PyTorch model provided as its first argument, recording a trace of what operators are used during the execution, and then converts those operators into ONNX equivalents. Because `torch.onnx.export` runs the model, we need to provide the function with an input tensor as its second argument, which can be random so long as it satisfies the shape and type of the model's input. As of Furiosa SDK v0.6, ONNX OpSet 12 is the most well-supported version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad941fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dummy input of the shape, (1, 3, 224, 224), of the model's input.\n",
    "dummy_input = (torch.randn(1, 3, 224, 224),)\n",
    "\n",
    "# Export the PyTorch model into an ONNX model.\n",
    "torch.onnx.export(\n",
    "    torch_model,  # PyTorch model to export\n",
    "    dummy_input,  # model input\n",
    "    \"resnet50.onnx\",  # where to save the exported ONNX model\n",
    "    opset_version=13,  # the ONNX OpSet version to export the model to\n",
    "    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "    input_names=[\"input\"],  # the ONNX model's input names\n",
    "    output_names=[\"output\"],  # the ONNX model's output names\n",
    ")\n",
    "\n",
    "# Load the exported ONNX model.\n",
    "onnx_model = onnx.load_model(\"resnet50.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0021d",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b07456",
   "metadata": {},
   "source": [
    "We will use subsets of the ImageNet dataset for calibration and validation. \n",
    "\n",
    "You need to download `ILSVRC2012_img_val.tar` and `ILSVRC2012_devkit_t12.tar.gz` externally and place them in the `imagenet` directory. Torchvision cannot download the ImageNet dataset automatically because it is no longer publicly accessible: https://github.com/pytorch/vision/pull/1457.\n",
    "\n",
    "Note that it may take several minutes to run this step for the first time because it involves decompressing the archive files. It will take much less time to complete subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "637a0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet = torchvision.datasets.ImageNet(\"imagenet\", split=\"val\", transform=preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a125399e",
   "metadata": {},
   "source": [
    "## Calibrate and Quantize ONNX Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5a7038",
   "metadata": {},
   "source": [
    "For quick demonstration, a small number of samples randomly chosen from the ImageNet dataset is used for calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7244e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_dataset = torch.utils.data.Subset(imagenet, torch.randperm(len(imagenet))[:100])\n",
    "calibration_dataloader = torch.utils.data.DataLoader(calibration_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f256d4",
   "metadata": {},
   "source": [
    "We call the `optimize_model` function to optimize onnx model, before calibration/quantization of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da4681da",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = optimize_model(onnx_model)\n",
    "onnx_model = onnx_model.SerializeToString()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1462c219",
   "metadata": {},
   "source": [
    "We use Calibrator to calibrate the model with various CalibrationMethod (e.g. MIN_MAX, Entropy, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6989ad40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Calibrator in module builtins:\n",
      "\n",
      "class Calibrator(object)\n",
      " |  Calibrator(model, calibration_method, percentage=99.99)\n",
      " |  \n",
      " |  A calibrator, which collects the values of tensors in an ONNX model\n",
      " |  and computes their ranges.\n",
      " |  \n",
      " |  Args:\n",
      " |      model (bytes): An ONNX model to calibrate.\n",
      " |      calibration_method (CalibrationMethod): A calibration method.\n",
      " |      percentage (float, optional): A percentage to use with\n",
      " |          percentile calibration. Defaults to 99.99 (i.e.\n",
      " |          99.99%-percentile calibration).\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  collect_data(self, calibration_dataset)\n",
      " |      Collect the values of tensors that will be used for range\n",
      " |      computation.\n",
      " |      \n",
      " |      This can be called multiple times.\n",
      " |      \n",
      " |      Args:\n",
      " |          calibration_dataset (Iterable[Sequence[numpy.ndarray]]):\n",
      " |              An object that provides input data for the model one at\n",
      " |              a time.\n",
      " |  \n",
      " |  compute_range(self, verbose)\n",
      " |      Estimate the ranges of the tensors on the basis of the collected\n",
      " |      data.\n",
      " |      \n",
      " |      Args:\n",
      " |          verbose (bool): Whether to show a progress bar, Defaults to\n",
      " |              False.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Calibrator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f747bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CalibrationMethod in module furiosa_quantizer_impl:\n",
      "\n",
      "class CalibrationMethod(enum.IntEnum)\n",
      " |  CalibrationMethod(value, names=None, *, module=None, qualname=None, type=None, start=1)\n",
      " |  \n",
      " |  Calibration method.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      MIN_MAX (CalibrationMethod): Min-max calibration.\n",
      " |      ENTROPY (CalibrationMethod): Entropy calibration.\n",
      " |      PERCENTILE (CalibrationMethod): Percentile calibration.\n",
      " |      MSE (CalibrationMethod): Mean squared error (MSE) calibration.\n",
      " |      SQNR (CalibrationMethod): Signal-to-quantization-noise ratio (SQNR) calibration.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CalibrationMethod\n",
      " |      enum.IntEnum\n",
      " |      builtins.int\n",
      " |      enum.Enum\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  ENTROPY = <CalibrationMethod.ENTROPY: 1>\n",
      " |  \n",
      " |  MIN_MAX = <CalibrationMethod.MIN_MAX: 0>\n",
      " |  \n",
      " |  MSE = <CalibrationMethod.MSE: 3>\n",
      " |  \n",
      " |  PERCENTILE = <CalibrationMethod.PERCENTILE: 2>\n",
      " |  \n",
      " |  SQNR = <CalibrationMethod.SQNR: 4>\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from enum.Enum:\n",
      " |  \n",
      " |  name\n",
      " |      The name of the Enum member.\n",
      " |  \n",
      " |  value\n",
      " |      The value of the Enum member.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from enum.EnumMeta:\n",
      " |  \n",
      " |  __members__\n",
      " |      Returns a mapping of member name->value.\n",
      " |      \n",
      " |      This mapping lists all enum members, including aliases. Note that this\n",
      " |      is a read-only view of the internal mapping.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CalibrationMethod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a51835",
   "metadata": {},
   "source": [
    "Before the Calibrator actually computes the ranges, input data should be collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df9f8abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrator = Calibrator(onnx_model, CalibrationMethod.MIN_MAX)\n",
    "\n",
    "calibrator.collect_data([calibration_data.numpy()] for calibration_data, _ in calibration_dataloader)\n",
    "for calibration_data, _ in calibration_dataloader:\n",
    "    calibrator.collect_data([[calibration_data.numpy()]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61d9e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = calibrator.compute_range()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86b4ce95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'/layer2/layer2.2/relu_1/Relu_output_0': (0.0, 39.90517807006836), '/layer1/layer1.0/conv1/Conv_output_0': (-47.21926498413086, 17.741018295288086), '/layer4/layer4.2/Add_output_0': (-38.632232666015625, 75.21089935302734), '/layer3/layer3.5/conv3/Conv_output_0': (-32.19559097290039, 16.876602172851562), '/layer3/layer3.2/conv2/Conv_output_0': (-30.72959327697754, 26.510149002075195), '/layer1/layer1.0/relu_2/Relu_output_0': (0.0, 24.1777400970459), '/layer2/layer2.2/relu/Relu_output_0': (0.0, 86.73884582519531), '/layer1/layer1.1/conv1/Conv_output_0': (-72.26683044433594, 20.372543334960938), '/layer2/layer2.0/Add_output_0': (-59.57727813720703, 92.01126098632812), '/layer3/layer3.1/conv1/Conv_output_0': (-48.180213928222656, 47.666568756103516), '/layer4/layer4.1/conv2/Conv_output_0': (-33.69670486450195, 18.930652618408203), '/layer3/layer3.1/conv3/Conv_output_0': (-120.35619354248047, 56.40264892578125), 'output': (-2.4086153507232666, 9.489575386047363), '/layer3/layer3.4/relu/Relu_output_0': (0.0, 57.39030075073242), '/layer3/layer3.4/conv2/Conv_output_0': (-45.717140197753906, 31.380704879760742), '/layer3/layer3.2/Add_output_0': (-63.79071044921875, 54.25042724609375), '/layer4/layer4.1/relu/Relu_output_0': (0.0, 32.1477165222168), '/layer1/layer1.0/conv2/Conv_output_0': (-21.531356811523438, 24.859193801879883), '/layer1/layer1.1/Add_output_0': (-63.48460006713867, 23.94207191467285), '/layer3/layer3.0/relu_2/Relu_output_0': (0.0, 98.6470947265625), '/layer3/layer3.5/conv2/Conv_output_0': (-23.594934463500977, 21.335988998413086), '/layer4/layer4.2/conv3/Conv_output_0': (-56.80466079711914, 32.84595489501953), '/layer3/layer3.2/relu/Relu_output_0': (0.0, 26.960622787475586), '/layer4/layer4.1/relu_1/Relu_output_0': (0.0, 18.930652618408203), '/layer3/layer3.0/relu/Relu_output_0': (0.0, 50.494651794433594), '/layer1/layer1.2/conv1/Conv_output_0': (-32.163482666015625, 20.832427978515625), '/layer4/layer4.1/Add_output_0': (-33.73779296875, 79.369873046875), '/layer3/layer3.4/conv1/Conv_output_0': (-19.31486701965332, 57.39030075073242), '/layer3/layer3.1/relu_1/Relu_output_0': (0.0, 50.19179153442383), '/layer2/layer2.3/Add_output_0': (-92.61951446533203, 159.69883728027344), '/layer3/layer3.0/conv1/Conv_output_0': (-57.544044494628906, 50.494651794433594), '/layer3/layer3.0/downsample/downsample.0/Conv_output_0': (-53.066932678222656, 79.16947174072266), '/layer3/layer3.3/relu_2/Relu_output_0': (0.0, 62.72626495361328), '/layer2/layer2.1/conv3/Conv_output_0': (-112.09184265136719, 111.04383087158203), '/layer2/layer2.1/relu_1/Relu_output_0': (0.0, 44.07295227050781), '/layer2/layer2.0/relu_2/Relu_output_0': (0.0, 92.01126098632812), '/layer2/layer2.1/Add_output_0': (-39.75624465942383, 178.8494873046875), '/layer3/layer3.1/conv2/Conv_output_0': (-78.02922821044922, 50.19179153442383), '/layer2/layer2.3/conv1/Conv_output_0': (-81.55775451660156, 86.46128845214844), '/layer1/layer1.1/relu_1/Relu_output_0': (0.0, 23.4921817779541), '/layer4/layer4.0/downsample/downsample.0/Conv_output_0': (-43.187225341796875, 96.03807067871094), '/layer4/layer4.1/conv3/Conv_output_0': (-44.374473571777344, 54.85395812988281), '/maxpool/MaxPool_output_0': (0.0, 39.82171630859375), '/layer2/layer2.2/conv1/Conv_output_0': (-116.9522705078125, 86.73884582519531), '/layer4/layer4.0/conv3/Conv_output_0': (-44.296958923339844, 47.81550979614258), '/layer1/layer1.0/downsample/downsample.0/Conv_output_0': (-66.01043701171875, 28.043088912963867), '/layer2/layer2.0/downsample/downsample.0/Conv_output_0': (-36.29875183105469, 21.788381576538086), '/layer2/layer2.3/relu_1/Relu_output_0': (0.0, 38.37862777709961), '/layer1/layer1.0/relu/Relu_output_0': (0.0, 17.741018295288086), '/layer3/layer3.0/conv2/Conv_output_0': (-24.301246643066406, 21.253883361816406), '/layer3/layer3.5/relu_2/Relu_output_0': (0.0, 42.27032470703125), '/layer1/layer1.0/Add_output_0': (-57.17456817626953, 24.1777400970459), '/layer2/layer2.3/relu_2/Relu_output_0': (0.0, 159.69883728027344), 'input': (-2.1179039478302, 2.640000104904175), '/layer1/layer1.0/conv3/Conv_output_0': (-19.4034366607666, 19.389842987060547), '/layer3/layer3.5/relu/Relu_output_0': (0.0, 22.712474822998047), '/layer1/layer1.1/relu_2/Relu_output_0': (0.0, 23.94207191467285), '/layer3/layer3.2/relu_1/Relu_output_0': (0.0, 26.510149002075195), '/layer4/layer4.0/relu_2/Relu_output_0': (0.0, 95.40895080566406), '/avgpool/GlobalAveragePool_output_0': (0.0, 9.393671989440918), '/layer1/layer1.2/conv2/Conv_output_0': (-28.324094772338867, 47.35013198852539), '/layer2/layer2.0/relu/Relu_output_0': (0.0, 47.501564025878906), '/layer2/layer2.3/conv2/Conv_output_0': (-96.8429946899414, 38.37862777709961), '/layer3/layer3.1/relu_2/Relu_output_0': (0.0, 67.73054504394531), '/layer2/layer2.2/Add_output_0': (-65.09489440917969, 180.85189819335938), '/layer3/layer3.3/conv1/Conv_output_0': (-20.06210708618164, 35.81584548950195), '/layer3/layer3.5/conv1/Conv_output_0': (-22.461395263671875, 22.712474822998047), '/layer2/layer2.0/conv2/Conv_output_0': (-24.003110885620117, 58.52951431274414), '/layer1/layer1.2/relu_2/Relu_output_0': (0.0, 129.07180786132812), '/layer3/layer3.3/relu/Relu_output_0': (0.0, 35.81584548950195), '/layer4/layer4.1/relu_2/Relu_output_0': (0.0, 79.369873046875), '/layer2/layer2.1/conv1/Conv_output_0': (-78.40660858154297, 94.96864318847656), '/relu/Relu_output_0': (0.0, 39.82171630859375), '/layer3/layer3.0/Add_output_0': (-48.89862823486328, 98.6470947265625), '/layer3/layer3.1/Add_output_0': (-90.8669662475586, 67.73054504394531), '/layer1/layer1.1/conv2/Conv_output_0': (-26.995359420776367, 23.4921817779541), '/layer3/layer3.2/relu_2/Relu_output_0': (0.0, 54.25042724609375), '/layer3/layer3.0/conv3/Conv_output_0': (-25.269289016723633, 37.18184280395508), '/layer2/layer2.0/conv3/Conv_output_0': (-39.13734436035156, 81.91060638427734), '/layer3/layer3.3/conv3/Conv_output_0': (-60.906192779541016, 62.332252502441406), '/layer3/layer3.4/relu_2/Relu_output_0': (0.0, 48.72160339355469), '/layer4/layer4.0/conv2/Conv_output_0': (-19.82232666015625, 16.909605026245117), '/layer3/layer3.2/conv1/Conv_output_0': (-24.341716766357422, 26.960622787475586), 'output_fused': (-2.4086153507232666, 9.489575386047363), '/layer3/layer3.4/Add_output_0': (-55.02952194213867, 48.72160339355469), '/layer3/layer3.3/conv2/Conv_output_0': (-28.109764099121094, 41.76040267944336), '/layer1/layer1.0/relu_1/Relu_output_0': (0.0, 24.859193801879883), '/layer1/layer1.2/Add_output_0': (-42.28086471557617, 129.07180786132812), '/layer3/layer3.0/relu_1/Relu_output_0': (0.0, 21.253883361816406), '/layer2/layer2.1/relu/Relu_output_0': (0.0, 94.96864318847656), '/layer4/layer4.2/relu/Relu_output_0': (0.0, 31.653411865234375), '/layer1/layer1.2/relu/Relu_output_0': (0.0, 20.832427978515625), '/layer2/layer2.2/conv2/Conv_output_0': (-77.58203887939453, 39.90517807006836), '/layer2/layer2.1/conv2/Conv_output_0': (-32.71854019165039, 44.07295227050781), '/layer3/layer3.3/relu_1/Relu_output_0': (0.0, 41.76040267944336), '/layer1/layer1.1/conv3/Conv_output_0': (-63.48460006713867, 21.324758529663086), '/layer2/layer2.3/conv3/Conv_output_0': (-92.61951446533203, 30.16446876525879), '/layer3/layer3.4/conv3/Conv_output_0': (-55.02952194213867, 40.205963134765625), '/layer4/layer4.0/relu_1/Relu_output_0': (0.0, 16.909605026245117), '/layer4/layer4.2/relu_1/Relu_output_0': (0.0, 16.588363647460938), '/layer4/layer4.0/relu/Relu_output_0': (0.0, 27.21522331237793), '/layer4/layer4.2/conv1/Conv_output_0': (-44.02098083496094, 31.653411865234375), '/layer2/layer2.0/conv1/Conv_output_0': (-48.83077621459961, 47.501564025878906), '/layer2/layer2.0/relu_1/Relu_output_0': (0.0, 58.52951431274414), '/layer4/layer4.1/conv1/Conv_output_0': (-30.54570198059082, 32.1477165222168), '/layer3/layer3.3/Add_output_0': (-60.906192779541016, 62.72626495361328), '/layer2/layer2.3/relu/Relu_output_0': (0.0, 86.46128845214844), '/layer4/layer4.2/conv2/Conv_output_0': (-39.823280334472656, 16.588363647460938), '/layer1/layer1.2/conv3/Conv_output_0': (-42.28086471557617, 129.07180786132812), '/layer4/layer4.0/conv1/Conv_output_0': (-22.694971084594727, 27.21522331237793), '/layer4/layer4.0/Add_output_0': (-52.46186828613281, 95.40895080566406), '/layer3/layer3.5/relu_1/Relu_output_0': (0.0, 21.335988998413086), '/layer3/layer3.5/Add_output_0': (-32.19559097290039, 42.27032470703125), '/layer3/layer3.4/relu_1/Relu_output_0': (0.0, 31.380704879760742), '/conv1/Conv_output_0': (-34.0945930480957, 39.82171630859375), '/layer1/layer1.2/relu_1/Relu_output_0': (0.0, 47.35013198852539), '/layer2/layer2.2/conv3/Conv_output_0': (-65.09489440917969, 28.70229148864746), '/layer2/layer2.2/relu_2/Relu_output_0': (0.0, 180.85189819335938), '/layer3/layer3.2/conv3/Conv_output_0': (-63.79071044921875, 26.410001754760742), '/layer3/layer3.1/relu/Relu_output_0': (0.0, 47.666568756103516), '/layer4/layer4.2/relu_2/Relu_output_0': (0.0, 75.21089935302734), '/layer2/layer2.1/relu_2/Relu_output_0': (0.0, 178.8494873046875), '/layer1/layer1.1/relu/Relu_output_0': (0.0, 20.372543334960938)}\n"
     ]
    }
   ],
   "source": [
    "print(ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df4392b",
   "metadata": {},
   "source": [
    "With the range computed, now we can quantize the model by calling `optimize_model` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97761424",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = quantize(onnx_model, ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "361a1045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function quantize in module furiosa_quantizer_impl.quantizer_impl:\n",
      "\n",
      "quantize(model, tensor_name_to_range, with_quantize)\n",
      "    Quantize an ONNX model on the basis of the range of its tensors.\n",
      "    \n",
      "    Args:\n",
      "        model (bytes): An ONNX model to quantize.\n",
      "        tensor_name_to_range (Mapping[str, Sequence[float]]):\n",
      "            A mapping from a tensor name to a 2-tuple (or list) of the\n",
      "            tensor's min and max.\n",
      "        with_quantize (bool): Whether to put a Quantize operator at the\n",
      "            beginning of the resulting model. Defaults to True.\n",
      "    \n",
      "    Returns:\n",
      "        Graph: An intermediate representation (IR) of the quantized\n",
      "            model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(quantize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec12dd6",
   "metadata": {},
   "source": [
    "The process above can be done in one-shot, by using `post_training_quantize` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c5d21e",
   "metadata": {},
   "source": [
    "### Quantization in one-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2fa47b",
   "metadata": {},
   "source": [
    "As we did in the previous section, onnx model and dataset should be prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe74a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load_model(\"resnet50.onnx\")\n",
    "\n",
    "calibration_dataset = torch.utils.data.Subset(imagenet, torch.randperm(len(imagenet))[:100])\n",
    "calibration_dataloader = torch.utils.data.DataLoader(calibration_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341fc2c",
   "metadata": {},
   "source": [
    "dataset fed to the `post_training_quantize` function should be a type of `Iterable[Sequence[numpy.ndarray]]`.\n",
    "Outermost Iterable is for iteration over dataset, and Sequence is for inputs of model. (in case the model requires multiple inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d99e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label is not needed for calibration\n",
    "dataset = ([calibration_data.numpy()] for calibration_data, _ in calibration_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547bf60c",
   "metadata": {},
   "source": [
    "We call `post_training_quantize` function with onnx model and dataset to quantize model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de73c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = post_training_quantize(onnx_model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64833aa5",
   "metadata": {},
   "source": [
    "`post_training_quantize` function is a wrapper for several functionalites needed for quantization including optimization/serialization/calibration/quantization, which are explained in the previous section, and we can manually call those functions to have full control over the quantization phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87ca827e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function post_training_quantize in module furiosa.quantizer.utils:\n",
      "\n",
      "post_training_quantize(model: Union[bytes, str, pathlib.Path, onnx.onnx_ml_pb2.ModelProto], dataset: Iterable[Sequence[numpy.ndarray]], calibration_method: furiosa_quantizer_impl.CalibrationMethod = <CalibrationMethod.MIN_MAX: 0>, opset_version: int = 13, with_quantize: bool = True, verbose: bool = False) -> Graph\n",
      "    Conduct a quantization with given dataset and calibration method\n",
      "    Args:\n",
      "        model (bytes, str, Path, onnx.ModelProto): a byte string containing a model or\n",
      "            a path string of a onnx model or `onnx.ModelProto`\n",
      "        dataset: A calibration dataset.\n",
      "        calibration_method: A calibration method to use. (MIN_MAX, ENTROPY, etc.)\n",
      "            Defaults to MIN_MAX.\n",
      "        opset_version: ONNX OperatorSet version to use.\n",
      "            Defaults to 13.\n",
      "        with_quantize: Whether to put a Quantize operator at the\n",
      "            beginning of the resulting model. Defaults to True.\n",
      "        verbose: Whether to show calibration progress bar.\n",
      "            Defaults to False.\n",
      "    Returns:\n",
      "        Serialized quantized model that can be used to create a session.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(post_training_quantize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6616073d",
   "metadata": {},
   "source": [
    "## Run Inference with Quantized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6793276f",
   "metadata": {},
   "source": [
    "For quick demonstration, we use randomly chosen 1000 samples from the ImageNet dataset for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94d92bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2023-01-20T04:18:56.585783Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mnux::npu\u001b[0m\u001b[2m:\u001b[0m Npu (npu0pe0-1) is being initialized\n",
      "\u001b[2m2023-01-20T04:18:56.587575Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mnux\u001b[0m\u001b[2m:\u001b[0m NuxInner create with pes: [PeId(0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the compilation log into /root/.local/state/furiosa/logs/compile-20230120131856-oni7r9.log\n",
      "Using furiosa-compiler 0.9.0-dev (rev: 0e82d35da built at 2023-01-02T06:05:47Z)\n",
      "\u001b[1m\u001b[2m[1/5]\u001b[0m üîç   Compiling from dfg to ldfg\n",
      "Done in 71.27173s\n",
      "\u001b[1m\u001b[2m[2/5]\u001b[0m üîç   Compiling from ldfg to cdfg\n",
      "Done in 0.003129137s\n",
      "\u001b[1m\u001b[2m[3/5]\u001b[0m üîç   Compiling from cdfg to gir\n",
      "Done in 0.029114116s\n",
      "\u001b[1m\u001b[2m[4/5]\u001b[0m üîç   Compiling from gir to lir\n",
      "Done in 0.008903365s\n",
      "\u001b[1m\u001b[2m[5/5]\u001b[0m üîç   Compiling from lir to enf\n",
      "Done in 0.06838308s\n",
      "‚ú®  Finished in 71.38182s\n",
      "Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:07<00:00, 130.46images/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2023-01-20T04:20:18.032267Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mnux::npu\u001b[0m\u001b[2m:\u001b[0m NPU (npu0pe0-1) has been destroyed\n",
      "\u001b[2m2023-01-20T04:20:18.035431Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mnux::capi\u001b[0m\u001b[2m:\u001b[0m session has been destroyed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = torch.utils.data.Subset(imagenet, torch.randperm(len(imagenet))[:1000])\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=1)\n",
    "\n",
    "correct_predictions, total_predictions = 0, 0\n",
    "elapsed_time = 0\n",
    "with furiosa.runtime.session.create(bytes(graph)) as session:\n",
    "    for image, label in tqdm.tqdm(validation_dataloader, desc=\"Evaluation\", unit=\"images\", mininterval=0.5):\n",
    "        image = image.numpy()\n",
    "        start = time.perf_counter_ns()\n",
    "        outputs = session.run(image)\n",
    "        elapsed_time += time.perf_counter_ns() - start\n",
    "        \n",
    "        prediction = np.argmax(outputs[0].numpy(), axis=1)  # postprocessing  \n",
    "        if prediction == label.numpy():\n",
    "            correct_predictions += 1\n",
    "        total_predictions += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2d16e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.600000%\n",
      "Average Latency: 2.408617062 ms\n"
     ]
    }
   ],
   "source": [
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Accuracy: {accuracy:%}\")\n",
    "\n",
    "latency = elapsed_time / total_predictions\n",
    "print(f\"Average Latency: {latency / 1_000_000} ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
